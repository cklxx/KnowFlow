# KnowFlow Docker Compose Environment Variables

# ===== Backend Configuration =====

# LLM Provider: "remote" (OpenAI-compatible) or "ollama" (local)
LLM_PROVIDER=remote

# LLM Request Timeout (seconds)
LLM_TIMEOUT_SECS=30

# Remote LLM API Configuration (OpenAI-compatible)
# Replace with your own API credentials
LLM_API_BASE=https://api.openai.com
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=your-api-key-here
LLM_MAX_TOKENS=4096

# Ollama Configuration (uncomment if using local provider)
# OLLAMA_API_BASE=http://127.0.0.1:11434
# OLLAMA_MODEL=llama3
# OLLAMA_KEEP_ALIVE=30m
# OLLAMA_TEMPERATURE=0.2
# OLLAMA_TOP_P=0.95
# OLLAMA_REPEAT_PENALTY=1.08

# ===== Frontend Configuration =====

# API Base URL for frontend (leave as default for Docker internal network)
# VITE_API_BASE_URL=http://backend:3000
